# ğŸ› ï¸ Data Engineering Pipeline: API to DuckDB with Airflow

This repository contains a simple and modular data engineering project that demonstrates how to:
- Extract data from an API
- Transform and validate it
- Load it into a local **DuckDB** database
- Orchestrate the entire process with **Apache Airflow**

It's ideal for beginners or junior data engineers looking to practice core ETL skills using modern tools.

---

## ğŸ“ Project Structure

```bash
DE-Fold/
â”œâ”€â”€ airflow/                   # Airflow DAGs and configs
â”œâ”€â”€ degpt-env/                 # Python virtual environment (do not track in GitHub)
â”œâ”€â”€ logs/dag_processor/        # Airflow logs
â”œâ”€â”€ docker-compose.yaml        # Docker config to spin up Airflow
â”œâ”€â”€ Dockerfile                 # Dockerfile (optional: customize image)
â”œâ”€â”€ requirements.txt           # Python libraries required
â”œâ”€â”€ init/db/                   # Folder for DB init scripts
â”œâ”€â”€ init-db.py                 # Script to initialize DuckDB schema/tables
â”œâ”€â”€ ecommerce_data.duckdb      # DuckDB file with sample data
â”œâ”€â”€ etl_creation.log           # Log file tracking ETL runs
â””â”€â”€ testing-function.py
â””â”€â”€ README.md       
